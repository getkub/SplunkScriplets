Sizing App - https://splunk-sizing.appspot.com

search factor  - The number of searchable copies of data that an indexer cluster maintains.
On a multisite indexer cluster, a special version of the search factor, known as the site search factor, determines not only the number of searchable copies that the entire cluster maintains but also the number of copies that each site maintains.
http://docs.splunk.com/Documentation/Splunk/6.2.0/Indexer/Bucketsandclusters
If SF>1, the master also tells the peer which of its target peers should make its copy of the data searchable
 search factor determines the number of searchable copies of each bucket. (Default 2) .Also SF <= RF.

Replication Factor
- In the case of an indexer cluster, the number of copies of data that the cluster maintains. A cluster can tolerate a failure of (replication factor - 1) peer nodes.
- On a multisite indexer cluster, a special version of the replication factor, known as the site replication factor, determines not only the number of copies that the entire cluster maintains but also the number of copies that each site maintains.
- In the case of a search head cluster, the number of copies of each search artifact that the cluster maintains.
 
 The peer begins streaming the processed rawdata to the target peers specified by the master. 
 It does not wait until its rawdata file is complete to start streaming its contents; rather, it streams the rawdata in blocks, as it processes the incoming data. It also tells any target peers if they need to make their copies searchable
 
##### master's server.conf   - http://docs.splunk.com/Documentation/Splunk/latest/Indexer/Sitereplicationfactor
[clustering]
mode = master
multisite=true
available_sites=site1,site2
site_replication_factor = origin:2,total:3
site_search_factor = origin:1,total:2

 # site_replication_factor = origin:2, site1:1, site2:1, site3:1, total:4
A peer in site1 ingests the data, it will stream one copy of the data to another peer in site1 (to fulfill the origin setting of 2), one copy to a peer in site2, and one copy to a peer in site3.

# ERROR if not properly configured
ERROR ClusteringMgr - Failure to load cluster config (server.conf) Error = site_replication_factor={ origin:1, total:2 } is less than replication_factor=3.
 ====================================
1. Restart the master node, as you would any instance. For example, run this CLI command on the master:  "splunk restart"
2. Once the master restarts, wait until all the peers re-register with the master, and the master dashboard indicates that all peers and indexes are searchable.
3. Restart the peers as a group, by running this CLI command on the master:
splunk rolling-restart cluster-peers

-- The rolling restart works like this: The master issues a restart message to approximately 10% (by default) of the peer nodes at a time. (If there are less than 10 peers in the cluster, it issues the restart to one peer at a time.) Once those peers restart and contact the master, the master then issues a restart message to another 10% of the peers, and so on, until all the peers have restarted. master restarts the peers in random order. In the case of multsite clusters, this operation is not site-aware.
=======================================
Restart a single peer - All peers must use the same set of indexes.conf files
There are two ways that you can safely restart a single peer:
- Through Splunk Web (Settings>Server Controls).
- With the CLI command offline, followed by start.  (splunk offline ; splunk edit cluster-config -restart_timeout <seconds> # Extend restart period)
Do not use the CLI restart command to restart the peer, the master will not be aware that the peer is restarting
The offline command removes a peer from the cluster and then stops the peer. 
It takes the peer down gracefully, allowing any in-progress searches to complete while quickly returning the cluster to a fully searchable state.
=======================================
Remove a peer from the master's list
./splunk remove cluster-peers -peers <guid>,<guid>,<guid>,...
The GUIDs can be specified with or without hyphens. For example: 4EB4D230-CB8B-4DEB-AD68-CF9209A6868A and 4EB4D230CB8B4DEBAD68CF9209A6868A are both valid.
If any GUID on the list is invalid, because one of the GUIDs does not correlate to a downed peer, the master aborts the entire operation.
=======================================
Handle master site failure (Configure a stand-by master preparatory step)
If the site holding the master node fails, you can bring up a new master on one of the remaining sites. 
In the meantime, the cluster continues to function as best it can. 
The peers continue to stream data to other peers based on the list of target peers they were using at the time the master went down.

Restart indexing after master restart or site failure 
# command on the master to unblock indexing when replication factor number of peers are not available
splunk set indexing-ready -auth admin:changeme

When the master comes back up, it waits for a quiet period of 60 seconds, so that all peers have an opportunity to re-register
After the master has a complete view of the state of the cluster, including the state of peer nodes and buckets.
=======================================
http://docs.splunk.com/Documentation/Splunk/latest/Indexer/Clusterdeploymentoverview
 in a clustering environment, you should ordinarily enable indexer acknowledgment for each forwarder sending data to a peer
Search across a single-site cluster
- The search head contacts the master node.
- The master node gives the SH the current generation ID and a list of the peers in that generation
- The search head communicates with the search peers
For Multi-site, - Search affinity is always enabled with multisite clusters
=======================================
configuration bundle  (On the master)
Structure looks like
$SPLUNK_HOME/etc/apps/
    cluster_define_apps1
    cluster_define_apps2
    
$SPLUNK_HOME/etc/licenses/
$SPLUNK_HOME/etc/deployment-apps/     # if cluster manager is used as deployment manager
    normal_apps2
    normal_apps3

$SPLUNK_HOME/etc/master-apps/
     _cluster/
          default/
          local/
     <app-name>/
     <app-name>/
     
On the peers -> (same Structure) $SPLUNK_HOME/etc/slave-apps/

Apply the bundle to the peers (via Web -> Distribute Configuration Bundle option)
CLI -> splunk apply cluster-bundle --answer-yes ; splunk show cluster-bundle-status
=======================================
# All commands from CLM

# apply/push apps from cluster master
splunk apply cluster-bundle

# status of index cluster
splunk list cluster-peers
OR
splunk show cluster-bundle-status

# restart indexes in cluster
splunk rolling-restart cluster-peers
